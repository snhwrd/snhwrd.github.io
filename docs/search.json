[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "posts",
    "section": "",
    "text": "Record terminal commands with uvx and asciinema\n\n\n\ntools\n\npython\n\nuv\n\n\n\neasily make gifs for docs and tutorials\n\n\n\n\n\nJul 22, 2025\n\n\nSean Howard\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "posts"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/post.html",
    "href": "posts/post.html",
    "title": "Post With Code",
    "section": "",
    "text": "x = 10\nprint(10*x)\n\n100"
  },
  {
    "objectID": "posts/2022-09-18-Diffusing_Digits.html",
    "href": "posts/2022-09-18-Diffusing_Digits.html",
    "title": "Diffusing Digits",
    "section": "",
    "text": "Open in Colab\n\n\n\n\n\n\n\nGenerating MNIST digits with diffusion\n\n\nDiffusion models have become the state of the art generative model by learning how to progressively remove “noise” from a randomly generated noise field until the sample matches the training data distribution. Diffusion models are a fundamental part of several noteworthy text to image models, including Imagen, DALLE-2, and Stable Diffusion. However, they are capabilities beyond text to image generation and are applicable to a large variety of generative tasks.\nHere a minimal diffusion model is trained on the iconic MNIST Digits database using several HuggingFace libraries. The flow follows that of the example HuggingFace notebook for unconditional image generation. I chose HuggingFace libraries for the implementation to learn their framework and I found that they were a nice balance between coding everything up in raw PyTorch (as was done in HuggingFace annotated diffusion blog post) and tailored implementations such as Phil Wang’s denoising-diffusion-pytorch.\n\n\nDiffusion Models - Quick Explanation\n\nConceptually, diffusion models are built upon a series of noising and denoising steps. In the noising process, random Gaussian noise is iteratively added to data (typically an image but can be any numeric datatype). After many steps of adding noise, the original data becomes indistinguishable from Gaussian noise. This noising process is going from right to left in the below figure from the Denoising Diffusion Probabilistic Models paper (often referred to as DDPM). In practice, getting from the original data to the step \\[t\\] of the noising process can be done in one go based upon convenient properties of Gaussians.\n\n\n\nThe real juice of diffusion models is the denoising process. In the figure above, each denoising step (left to right in above figure), attempts to remove the noise added from previous step. Given noisy data, the diffusion model tries to predict the noise present in the data (slightly different to the above depiction which shows the model learning the conditional probability distribution \\[p(x_{t-1}\n\\vert x_t)\\]). This noise is iteratively removed until the denoised data, which by characteristic of the training distribution, is left.\nDiffusion models can be broken down into two algorithms, one for training and one for sampling.\n\n\nThe training algorithm is relatively simple and follow the steps - Take data from training distribution - Randomly select a step within the noisig/denoising process - Sample random Gaussian noise with zero mean and unit variance - Take noise field and data from training distribution and noise it to selected step from noising process. - Predict the noise present in the noisy data - Update model based upon mean squared error of actual noise and predicted noise\nWhich is shown in the psuedocode from the Ho et. al paper.\n\n\n\n\n\n\n\n\nWith a model that takes a noisy image and predicts the noise given the step in the noising chain, can iteratively denoise the data with the following steps - Generate the fully noised data at last step \\[T\\] - For each step in the chain, predict the noise in the image and remove some fraction of it.\nWhich is shown in the pseudocode\n\n\n\n\n\nThere are details about noise and learning rate schedules which were omitted from the above, but covered in the annotated diffusion blog post"
  },
  {
    "objectID": "posts/2022-09-18-Diffusing_Digits.html#outline",
    "href": "posts/2022-09-18-Diffusing_Digits.html#outline",
    "title": "Diffusing Digits",
    "section": "Outline",
    "text": "Outline\nIn creating a diffusion model with HuggingFace, I found there to be 4 main stages after choosing the hyperparameters, each with defined subtasks. I’ve shown an outline below\n\nDefining Hyperparameters \nPreparing Dataset\n\nDownloading MNIST with HuggingFace datasets\nData preprocessing and augmentation\n\nCreating the Diffusion Model\n\nU-Net for MNIST\nNoise Scheduler\nOptimizer\nLearning Rate Scheduler\n\nTraining the Model\n\nWorking with memory restrictions\nCreating and running training script\n\nSampling Images\n\n\n\nImport libraries\n\n# Pytorch\nimport torch\nimport torchvision\n\n# HuggingFace\nimport datasets\nimport diffusers\nimport accelerate\n\n# Training and Visualization\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport os\nimport PIL"
  },
  {
    "objectID": "posts/2022-09-18-Diffusing_Digits.html#defining-hyperparameters",
    "href": "posts/2022-09-18-Diffusing_Digits.html#defining-hyperparameters",
    "title": "Diffusing Digits",
    "section": "Defining Hyperparameters",
    "text": "Defining Hyperparameters\nIn the training config class shown below, I’ve chosen an image size of \\[32 \\times 32\\] instead of the default MNIST resolution of \\[28 \\times 28\\]. This slight upscaling is in order to make the image width/height be a power of 2, i.e. \\[ 2^5 \\]. In the default U-Net architecture, each downsampling layer reduces the width and height of the image by 2. Therefore after the three downsampling blocks I used in the U-Net, the output size will be \\[4 \\times 4 \\times N \\], where \\[N\\] is a configurable parameter of the model architecture. As the width and height of the image is reduced, the number of learned channels increases. So in the U-Net configured here, the bottleneck layer has dimension of \\[4 \\times 4 \\times 512\\].\nThe batch sizes chosen are done in order to comfortably fit on a 8 GB memory GPU. I find that training occupies approximately 4 GB of memory. Since one epoch contains all sixty thousand training examples, only a couple epochs are needed for the model to converge, with most of the learning being done within the first epoch.\nThe lr_warmup_steps is the number of mini-batches where the learning rate is increased until hitting the base learning rate listed in learning_rate. After the learning rate reaches this value, a cosine scheduler is used to slowly decrease the learning rate, as described in Improved Denoising Diffusion Probabilistic Models.\nfrom dataclasses import dataclass\n\n@dataclass\nclass TrainingConfig:\n    image_size=32 #Resize the digits to be a power of two\n    train_batch_size = 32\n    eval_batch_size = 32\n    num_epochs = 5\n    gradient_accumulation_steps = 1\n    learning_rate = 1e-4\n    lr_warmpup_steps = 500\n    mixed_precision = 'fp16'\n    seed = 0\n    \nconfig = TrainingConfig()"
  },
  {
    "objectID": "posts/2022-09-18-Diffusing_Digits.html#preparing-mnist-dataset",
    "href": "posts/2022-09-18-Diffusing_Digits.html#preparing-mnist-dataset",
    "title": "Diffusing Digits",
    "section": "Preparing MNIST Dataset",
    "text": "Preparing MNIST Dataset\n\nDownloading MNIST with HuggingFace datasets\nHuggingFace has almost ten thousand dataset for download, which can be searched from the datasets tab of their website. They can be downloaded with their datasets python library and the load_dataset() function.\nIf not specified, the data will be downloaded to the ~/.cache directory. If you want to put the files in another location, either specify the data_dir optional argument or change the environment variable HF_DATASETS_CACHE to the desired path.\nHere MNIST digits are loaded into a Dataset object, where metadata, labels, and images can be accessed in a manner similar to python dictionaries.\nmnist_dataset = datasets.load_dataset('mnist', split='train')\nThe dataset object is conveniently accessible with methods similar to a python dictionary\nmnist_dataset\nDataset({\n    features: ['image', 'label'],\n    num_rows: 60000\n})\nmnist_dataset[0][\"image\"].resize((256, 256)).show()\nprint(\"Image Size:\", mnist_dataset[0][\"image\"].size)\nprint(\"Digit is labelled:\", mnist_dataset[0]['label'])\n\n\n\nImage Size: (28, 28)\nDigit is labelled: 5\n\n\nData Preprocessing and Augmentation\nAs downloaded, the MNIST dataset contains 60,000 PIL images with pixel values in the range of \\[[0,255]\\]. The data must be scaled, resized, and turned into a tensor for ingestion by a PyTorch model. These transformations can be handled by torchvision’s transforms library. Transform objects can be sequentially listed in a Compose constructor, which will apply then apply the transformations when an image is passed as an argument.\nThree transforms are used. The first transforms the image to 32x32, in order to for the image width/height to be a power of two. The second transform turns the PIL image to a PyTorch tensor. When converting to a PyTorch tensor, the pixel range is transformed from \\[[0,255]\\] to \\[[0,1]\\]. However, for the diffusion model the required pixel value range needs to be \\[[-1,1]\\] since the Gaussian noise is zero mean, unit variance. Therefore, a lambda function is used to define a transform from \\[[0,1]\\] to \\[[-1,1]\\].\nThe Datasets object has a method set_transform() which applies a function which takes the dataset object as an argument. Here the method is used to apply the torchvision transforms to the MNIST dataset.\ndef transform(dataset):\n    preprocess = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.Resize(\n                (config.image_size, config.image_size)),\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Lambda(lambda x: 2*(x-0.5)),\n        ]\n    )\n    images = [preprocess(image) for image in dataset[\"image\"]]\n    return {\"images\": images}\nmnist_dataset.reset_format()\nmnist_dataset.set_transform(transform)\nOnce the dataset has been prepared with the proper transformers, it is ready to be passed directly into a PyTorch DataLoader.\ntrain_dataloader = torch.utils.data.DataLoader(\n    mnist_dataset,\n    batch_size = config.train_batch_size,\n    shuffle = True,\n)"
  },
  {
    "objectID": "posts/2022-09-18-Diffusing_Digits.html#creating-the-diffusion-model",
    "href": "posts/2022-09-18-Diffusing_Digits.html#creating-the-diffusion-model",
    "title": "Diffusing Digits",
    "section": "Creating the Diffusion Model",
    "text": "Creating the Diffusion Model\n\nU-Net for MNIST\nThe workhorse of the denoising diffusion model is a U-Net, which is predicts the noise present in the input image conditioned on the step in the noising process. HuggingFace’s Diffusers library has default a U-Net class which creates a PyTorch model based upon the input values. Here the input and output channels are set to one since the image is black and white. The rest of the parameters mirror the choices found in the example notebook from HuggingFace.\nmodel = diffusers.UNet2DModel(\n    sample_size=config.image_size,\n    in_channels=1,\n    out_channels=1,\n    layers_per_block=2,\n    block_out_channels=(128,128,256,512),\n    down_block_types=(\n        \"DownBlock2D\",\n        \"DownBlock2D\",\n        \"AttnDownBlock2D\",\n        \"DownBlock2D\",\n    ),\n    up_block_types=(\n        \"UpBlock2D\",\n        \"AttnUpBlock2D\",\n        \"UpBlock2D\",\n        \"UpBlock2D\",\n    ),\n)\nCheck that the input image to the model and the output have the same shape\nsample_image = mnist_dataset[0][\"images\"].unsqueeze(0)\nprint(\"Input shape:\", sample_image.shape)\nInput shape: torch.Size([1, 1, 32, 32])\nprint('Output shape:', model(sample_image, timestep=0)[\"sample\"].shape)\nOutput shape: torch.Size([1, 1, 32, 32])\n\n\nNoise Scheduler\nIn diffusion models, the noise is added to images dependent on the step within noising/denoising process. In the original DDPM paper, the strength of the noise added to the image (i.e. the variance of the zero mean Gaussian) increased linearly with time steps. The Diffusers library has a noise scheduler object which handles the amount of noise to be added for a given step. The default values for noise are taken from the DDPM paper, but there are optional arguments to change the starting and ending noise strength, along with the how the noise changes with across steps.\nnoise_scheduler = diffusers.DDPMScheduler(num_train_timesteps=200, tensor_format='pt')\nWe can take a digit and use the scheduler object to add noise. Below is the\nprint(\"Original Digit\")\ntorchvision.transforms.ToPILImage()(sample_image.squeeze(1)).resize((256,256))\nOriginal Digit\n\n\n\nnoise = torch.randn(sample_image.shape)\ntimesteps = torch.LongTensor([199])\nnoisy_image = noise_scheduler.add_noise(sample_image,noise,timesteps)\n\nprint(\"Fully Noised Digit\")\ntorchvision.transforms.ToPILImage()(noisy_image.squeeze(1)).resize((256,256))\nFully Noised Digit\n\n\n\n\n\nOptimizer\nLet’s have the U-Net can learn with the AdamW optimizer.\noptimizer = torch.optim.AdamW(model.parameters(),lr=config.learning_rate)\n\n\nLearning Rate Scheduler\nAs mentioned previously, in Improved Denoising Diffusion Probabilistic Models, they find a learning rate schedule which first warmups for a fixed number of steps and then follows a cosine schedule thereafter to be effective in training the model. The diffusers library has a method which creates a PyTorch learning rate scheduler which follows the advice given in this paper.\n# Cosine learning rate scheduler\n\nlr_scheduler = diffusers.optimization.get_cosine_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=config.lr_warmpup_steps,\n    num_training_steps=(len(train_dataloader)*config.num_epochs),\n)"
  },
  {
    "objectID": "posts/2022-09-18-Diffusing_Digits.html#training-the-model",
    "href": "posts/2022-09-18-Diffusing_Digits.html#training-the-model",
    "title": "Diffusing Digits",
    "section": "Training the Model",
    "text": "Training the Model\n\nWorking with memory restrictions\nRunning this on my local machine, I found that unless I set a limit on the VRAM accessible to PyTorch it would use it all up. This is good for maximizing utilization of a GPU cluster, but bad when iterating on a machine where the same GPU is rendering the operating system.\nTo get around this, there is a useful cuda function within PyTorch which sets the maximum fraction of total memory accessible. I’ve set this to use 7 GB out of 8 GB, just so computer doesn’t come to a standstill.\ntorch.cuda.set_per_process_memory_fraction(7./8., 0)\n\n\nCreating and Running Training Loop\nThe training function first creates a HuggingFace accelerator object. The purpose of the accelerator object is to automatically handle device assignment for PyTorch objects when training on multiple devices and to make the code portable when running in multiple setups. Once created, the accelerator has a method prepare which takes all of the model/U-Net, optimizer, dataloader, and learning rate scheduler and automatically detects the correct device(s) and makes the appropriate .to() assignments.\nAfter those objects are “prepared”, the training has an outer for loop for each epoch and an inner for loop for each mini-batch. In each mini-batch, a set of digits is taken from the dataset. Random noise with the same size of the minibatch is then sampled. Then, for each image in the minibatch, a random step in the noising process is (uniformly) selected. Noise is then added to each image based upon the randomly sampled noise and the randomly selected step. The U-Net then predicts the noise added to the image conditioned on the selected step. A mean squared error loss is then calculated between the predicted noise and the actual noise added to the image. This loss is then used to update the weights for each mini-batch.\ndef train_loop(\n        config,\n        model,\n        noise_scheduler,\n        optimizer,\n        train_dataloader,\n        lr_scheduler):\n\n    accelerator = accelerate.Accelerator(\n        mixed_precision=config.mixed_precision,\n        gradient_accumulation_steps=config.gradient_accumulation_steps,\n    )\n\n    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, lr_scheduler\n    )\n\n    for epoch in range(config.num_epochs):\n        progress_bar = tqdm(total=len(train_dataloader),\n                            disable=not accelerator.is_local_main_process)\n        progress_bar.set_description(f\"Epoch {epoch}\")\n\n        for step, batch in enumerate(train_dataloader):\n            clean_images = batch['images']\n\n            noise = torch.randn(clean_images.shape).to(clean_images.device)\n            batch_size = clean_images.shape[0]\n\n            # Sample a set of random time steps for each image in mini-batch\n            timesteps = torch.randint(\n                0, noise_scheduler.num_train_timesteps, (batch_size,), device=clean_images.device)\n            \n            noisy_images=noise_scheduler.add_noise(clean_images, noise, timesteps)\n            \n            with accelerator.accumulate(model):\n                noise_pred = model(noisy_images,timesteps)[\"sample\"]\n                loss = torch.nn.functional.mse_loss(noise_pred,noise)\n                accelerator.backward(loss)\n                \n                accelerator.clip_grad_norm_(model.parameters(),1.0)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                \n            progress_bar.update(1)\n            logs = {\n                \"loss\" : loss.detach().item(),\n                \"lr\" : lr_scheduler.get_last_lr()[0],\n            }\n            progress_bar.set_postfix(**logs)\n    \n    accelerator.unwrap_model(model)\nOnce the training loop set up, the function along with its arguments can be passed to the accelerate library’s notebook launcher to train within the notebook.\nargs = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n\naccelerate.notebook_launcher(train_loop, args, num_processes=1)"
  },
  {
    "objectID": "posts/2022-09-18-Diffusing_Digits.html#create-a-sampling-function",
    "href": "posts/2022-09-18-Diffusing_Digits.html#create-a-sampling-function",
    "title": "Diffusing Digits",
    "section": "Create a sampling function",
    "text": "Create a sampling function\nOnce the model has been trained, we can sample the model to create digits. Or more accurately create a sample which is within the learned distribution of the training samples, since some generated samples look like an alien’s numbering system, a mish-mash of the numbers 0-9.\nTo sample images, the Diffusers library has several pipelines. However, I found that these pipelines don’t work for single channel images. So I created a small function which samples the images, with an optional argument for saving off each step. Importantly, the function needs to have a torch.no_grad() decorator so the model doesn’t accumulate the history of the forward passes.\n@torch.no_grad()\ndef sample(unet, scheduler,seed,save_process_dir=None):\n    torch.manual_seed(seed)\n    \n    if save_process_dir:\n        if not os.path.exists(save_process_dir):\n            os.mkdir(save_process_dir)\n    \n    scheduler.set_timesteps(1000)\n    image=torch.randn((1,1,32,32)).to(model.device)\n    num_steps=max(noise_scheduler.timesteps).numpy()\n    \n    for t in noise_scheduler.timesteps:\n        model_output=unet(image,t)['sample']\n        image=scheduler.step(model_output,int(t),image,generator=None)['prev_sample']\n        if save_process_dir:\n            save_image=torchvision.transforms.ToPILImage()(image.squeeze(0))\n            save_image.resize((256,256)).save(\n                os.path.join(save_process_dir,\"seed-\"+str(seed)+\"_\"+f\"{num_steps-t.numpy():03d}\"+\".png\"),format=\"png\")\n        \n    return torchvision.transforms.ToPILImage()(image.squeeze(0))"
  },
  {
    "objectID": "posts/2022-09-18-Diffusing_Digits.html#sample-some-good-looking-digits",
    "href": "posts/2022-09-18-Diffusing_Digits.html#sample-some-good-looking-digits",
    "title": "Diffusing Digits",
    "section": "Sample some good looking digits!",
    "text": "Sample some good looking digits!\nSome samples look quit good…\ntest_image=sample(model,noise_scheduler,2)\ntest_image.resize((265,256))\n\n\n\ntest_image=sample(model,noise_scheduler,5)\ntest_image.resize((256,256))\n\n\n\ntest_image=sample(model,noise_scheduler,1991)\ntest_image.resize((256,256))\n\n\n\nBut others aren’t quite recognizable as a number, but look like they could be number if history went slightly differently…\ntest_image=sample(model,noise_scheduler,2022)\ntest_image.resize((256,256))\n\n\n\ntest_image=sample(model,noise_scheduler,42)\ntest_image.resize((256,256))"
  },
  {
    "objectID": "posts/dummy_post.html",
    "href": "posts/dummy_post.html",
    "title": "Post With Code",
    "section": "",
    "text": "x = 10\nprint(10*x)\n\n100"
  },
  {
    "objectID": "posts/092225_RecordTerminal/dummy_post.html",
    "href": "posts/092225_RecordTerminal/dummy_post.html",
    "title": "Record terminal commands with uvx and asciinema",
    "section": "",
    "text": "Often it is useful to record a terminal session, either to provide instructions for basic CLI usage, document getting started, or to share a bug with a team. Screen capture tools are often clunky and capture more than just the terminal. The popular asciicinema package provides a library for recording terminal sessions. But this saves the session in a specialized .cast file format, where often I would rather have a gif to post to a chat or link in a website. This can be done with the agg utility, but requires an additional step and remembering the command line arguments.\nAlso, as someone who develops in python I’d like to use the asciinema tool without having it in my virtual environment. This is where the uv and the uvx command comes in. uvx allows running a python library’s cli by managing the creation and teardown of a virtual environment. Because of uv’s speed and local caching of whl files, the process is barely noticeable.\nCombining these two tools, I created a script with a couple rounds of AI feedback that I can drop in my ~/.zshrc file.\n\n\nShow the code\n\n# Function to record an asciinema session and convert it to a GIF\n# Usage: recgif &lt;output_gif_name&gt; [asciinema_rec_options...]\nrecgif() {\n    if [ -z \"$1\" ]; then\n        echo \"Usage: recgif &lt;output_gif_name&gt; [asciinema_rec_options...]\"\n        return 1\n    fi\n\n    local output_gif_name_raw=\"$1\"\n    shift # Remove the first argument (output_gif_name_raw) from the argument list\n    local asciinema_rec_options=(\"$@\") # Capture all remaining arguments for asciinema rec\n\n    local output_gif_name # This will store the name with .gif only if needed\n    local temp_cast_file\n\n    # Check if the output_gif_name_raw already ends with .gif\n    if [[ \"$output_gif_name_raw\" == *.gif ]]; then\n        output_gif_name=\"$output_gif_name_raw\"\n    else\n        output_gif_name=\"${output_gif_name_raw}.gif\"\n    fi\n\n    # Create a unique temporary file for the asciinema cast.\n    # The temporary file name will now include the base output name as a prefix.\n    temp_cast_file=$(mktemp --tmpdir=\"${TMPDIR:-/tmp}\" \"${output_gif_name_raw}.XXXXXX.cast\")\n\n    echo \"Recording asciinema session. Press Ctrl+D or type 'exit' to finish...\"\n    # Record the asciinema session using uvx, saving to the temporary file.\n    # All additional arguments passed to recgif are forwarded to asciinema rec.\n    # This command blocks until the recording is finished (user presses Ctrl+D or types 'exit').\n    uvx asciinema rec \"${asciinema_rec_options[@]}\" \"$temp_cast_file\"\n\n    if [ -f \"$temp_cast_file\" ]; then\n        echo \"Converting cast to GIF: ${output_gif_name}\"\n        # Convert the asciinema cast to a GIF using agg directly,\n        # with the specified font family.\n        agg --font-family \"MesloLGS Nerd Font Mono\" --theme \"monaki\" \"$temp_cast_file\" \"$output_gif_name\"\n\n        echo \"Cleaning up temporary cast file: ${temp_cast_file}\"\n        # Remove the temporary asciinema cast file\n        rm \"$temp_cast_file\"\n        echo \"GIF created successfully: ${output_gif_name}\"\n    else\n        echo \"Asciinema recording failed or cast file not found. No GIF was created.\"\n    fi\n}"
  },
  {
    "objectID": "posts/092225_RecordTerminal/record_terminal.html",
    "href": "posts/092225_RecordTerminal/record_terminal.html",
    "title": "Record terminal commands with uvx and asciinema",
    "section": "",
    "text": "Often it is useful to record a terminal session, either to provide instructions for basic CLI usage, document getting started, or to share a bug with a team. Screen capture tools are often clunky and capture more than just the terminal. The popular asciicinema package provides a library for recording brief terminal sessions. But this saves the session in a specialized .cast file format, where often I would rather have a gif to post to a chat or link in a website. This can be done with the agg utility, but requires an additional step and remembering the command line arguments.\nAlso, as someone who develops in python I’d like to use the asciinema tool without having it in my virtual environment. This is where the uv and the uvx command comes in. uvx allows running a python library’s cli by managing the creation and teardown of a virtual environment. Because of uv’s speed and local caching of whl files, the process is barely noticeable.\nCombining these two tools, I created a script with a couple rounds of AI feedback that I can drop in my ~/.zshrc file.\n\n\nShow the code\n\n# Function to record an asciinema session and convert it to a GIF\n# Usage: recgif &lt;output_gif_name&gt; [asciinema_rec_options...]\nrecgif() {\n    if [ -z \"$1\" ]; then\n        echo \"Usage: recgif &lt;output_gif_name&gt; [asciinema_rec_options...]\"\n        return 1\n    fi\n\n    local output_gif_name_raw=\"$1\"\n    shift # Remove the first argument (output_gif_name_raw) from the argument list\n    local asciinema_rec_options=(\"$@\") # Capture all remaining arguments for asciinema rec\n\n    local output_gif_name # This will store the name with .gif only if needed\n    local temp_cast_file\n\n    # Check if the output_gif_name_raw already ends with .gif\n    if [[ \"$output_gif_name_raw\" == *.gif ]]; then\n        output_gif_name=\"$output_gif_name_raw\"\n    else\n        output_gif_name=\"${output_gif_name_raw}.gif\"\n    fi\n\n    # Create a unique temporary file for the asciinema cast.\n    # The temporary file name will now include the base output name as a prefix.\n    temp_cast_file=$(mktemp --tmpdir=\"${TMPDIR:-/tmp}\" \"${output_gif_name_raw}.XXXXXX.cast\")\n\n    echo \"Recording asciinema session. Press Ctrl+D or type 'exit' to finish...\"\n    # Record the asciinema session using uvx, saving to the temporary file.\n    # All additional arguments passed to recgif are forwarded to asciinema rec.\n    # This command blocks until the recording is finished (user presses Ctrl+D or types 'exit').\n    uvx asciinema rec \"${asciinema_rec_options[@]}\" \"$temp_cast_file\"\n\n    if [ -f \"$temp_cast_file\" ]; then\n        echo \"Converting cast to GIF: ${output_gif_name}\"\n        # Convert the asciinema cast to a GIF using agg directly,\n        # with the specified font family.\n        agg --font-family \"MesloLGS Nerd Font Mono\" --theme \"monaki\" \"$temp_cast_file\" \"$output_gif_name\"\n\n        echo \"Cleaning up temporary cast file: ${temp_cast_file}\"\n        # Remove the temporary asciinema cast file\n        rm \"$temp_cast_file\"\n        echo \"GIF created successfully: ${output_gif_name}\"\n    else\n        echo \"Asciinema recording failed or cast file not found. No GIF was created.\"\n    fi\n}\n\nHere it is in action:\n\n\n\nExample gif"
  }
]